{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyomee2/sswu-lc5500-aiapp/blob/main/Llama/models/llama_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgpavbRjWifl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. github 클론"
      ],
      "metadata": {
        "id": "OLyNMt0Xrx5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/hyomee2/sswu-lc5500-aiapp.git"
      ],
      "metadata": {
        "id": "KPYjcpeFW7aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd sswu-lc5500-aiapp"
      ],
      "metadata": {
        "id": "5GdIA5_uEEha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "2d1Z1MsODktO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 데이터셋 불러와서 훈련 포맷으로 맞추기"
      ],
      "metadata": {
        "id": "MRWaGxMBrwHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/sswu-lc5500-aiapp/Llama/dataset/Llama_data.csv\").dropna(subset=[\"Q\", \" A\"]) # 결측치 제외 후 불러오기\n",
        "\n",
        "# JSON 리스트 만들기\n",
        "records = []\n",
        "for _, row in df.iterrows():\n",
        "    records.append({\n",
        "        \"instruction\": row[\"Q\"].strip(), # 질문 앞뒤 공백 제거\n",
        "        \"input\": \"\", # input은 비움\n",
        "        \"output\": str(row[\" A\"]).strip() # A 앞에 들어있는 공백 제거\n",
        "    })\n",
        "\n",
        "# JSON 저장\n",
        "with open(\"/content/gdrive/MyDrive/aiapplication/train_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(records, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "JJg-D6MjnEor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Huggingface 데이터셋으로 변환 및 로딩"
      ],
      "metadata": {
        "id": "_6mqWuXksZhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JSON을 pandas로 읽기\n",
        "df = pd.read_json(\"/content/gdrive/MyDrive/aiapplication/train_data.json\")\n",
        "\n",
        "# Hugging Face Dataset으로 변환\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# train/validation 90:10으로 나누기\n",
        "split_dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "val_dataset = split_dataset[\"test\"]"
      ],
      "metadata": {
        "id": "bLJBhv_wtrmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. HuggingFace 모델 불러오기"
      ],
      "metadata": {
        "id": "Z9TFQwN4t65Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate peft bitsandbytes datasets"
      ],
      "metadata": {
        "id": "aL3XOm32tVSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D0gtW0Fkgq-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hugging_face_token = userdata.get(\"HUGGING_FACE_TOKEN\")\n",
        "login(hugging_face_token)"
      ],
      "metadata": {
        "id": "4ibG0lZwwEuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "base_model_name = \"beomi/Llama-3-Open-Ko-8B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # pad_token 설정\n",
        "tokenizer.padding_side = \"left\" # Llama는 autoregressive 모델이어서 오른쪽부터 단어를 예측하므로, 왼쪽 padding이 자연스럽다.\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    load_in_8bit=True, # LoRA를 위해 8bit 로딩\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "VKBhcMXbruvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "\n",
        "# LoRA를 위한 준비\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA 설정\n",
        "lora_config = LoraConfig(\n",
        "    r=16, # LoRA의 랭크(저차원 행렬의 크기). 보통 4~16 사이로 설정\n",
        "    lora_alpha=32, # LoRA 내 스케일링 팩터. 학습 안정성에 도움\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # LoRA가 어떤 신경망 내부 레이어(모듈)에만 적용될지 지정\n",
        "    lora_dropout=0.05, # 과적합 방지를 위해 LoRA 적용 시 드롭아웃 확률\n",
        "    bias=\"none\", # bias 파라미터 업데이트 여부(none, all, lora 등)\n",
        "    task_type=\"CAUSAL_LM\" # 작업 유형 지정. 여기서는 언어 생성용 인과적 언어모델\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n"
      ],
      "metadata": {
        "id": "gQKOyeoxuNXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** target_modules=[\"q_proj\", \"v_proj\"]**\n",
        "\n",
        "**Q, K, V란?**\n",
        "\n",
        "Transformer 모델의 핵심인 어텐션(attention) 계산에서\n",
        "Q: Query (질문)\n",
        "K: Key (키)\n",
        "V: Value (값)\n",
        "\n",
        "이 세 개 행렬(벡터)을 만들어서 어텐션 점수를 계산.\n",
        "\n",
        "\n",
        "**\"q_proj\", \"v_proj\"는 무엇인가?**\n",
        "\n",
        "Transformer 구현체마다 다르지만, 보통 Q, K, V를 만들 때\n",
        "q_proj, k_proj, v_proj라는 이름으로 각각의 선형 변환(Linear layer, 즉 행렬곱)을 만든다.\n",
        "\n",
        "e.g.,\n",
        "``` python\n",
        "self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "```\n",
        "\n",
        "\n",
        "**그럼 target_modules=[\"q_proj\", \"v_proj\"]는?**\n",
        "\n",
        "LoRA를 Q 프로젝션 레이어와 V 프로젝션 레이어에만 적용하겠다는 의미. 즉, Q와 V를 만드는 선형 변환에 LoRA 어댑터가 붙어서 학습된다. K는 빼고 Q, V에만 적용하는 건 실험적으로 성능이나 효율이 좋다고 알려진 경우가 많다. Q는 모델이 \"무엇을 집중할지\"를 결정하는 쿼리 정보고, V는 실제로 어텐션에서 참고하는 값이라, 이 두 부분을 조정하는 것이 모델 미세조정에 효과적일 때가 많다. K는 상대적으로 업데이트 효과가 적거나 중복될 수 있어서 제외하기도 한다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HH96rI3l0HxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 텍스트를 LLM 학습 포맷으로 변환"
      ],
      "metadata": {
        "id": "7xQ3DEMn8P7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt 템플릿 함수\n",
        "def format_prompt(example):\n",
        "    return f\"\"\"### 질문:\\n{example['instruction']}\\n\\n### 답변:\\n{example['output']}\"\"\""
      ],
      "metadata": {
        "id": "dKmLzP-F5nFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**format_prompt(example)**\n",
        "\n",
        "데이터를 LLM에 학습시키기 좋은 문자열 형태(Prompt)로 만들어준다.\n",
        "\n",
        "e.g.,\n",
        "\n",
        "```\n",
        "### 질문:\n",
        "\n",
        "12시 땡이다!\n",
        "\n",
        "### 답변:\n",
        "\n",
        "하루 또 가뿌네.\n",
        "```"
      ],
      "metadata": {
        "id": "B7udCq3P9o91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이징\n",
        "def tokenize(example):\n",
        "    prompt = format_prompt(example)\n",
        "    return tokenizer(prompt, padding=True, truncation=True, max_length=512)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize, remove_columns=train_dataset.column_names)\n",
        "tokenized_val = val_dataset.map(tokenize, remove_columns=val_dataset.column_names)"
      ],
      "metadata": {
        "id": "VxBs2pHz-KJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tokenize()**\n",
        "\n",
        "위에서 만든 문자열 prompt을 모델 학습용 숫자 토큰들로 변환한다.\n",
        "\n",
        "padding=\"max_length\": 길이가 512보다 짧으면 빈 자리를 채운다.\n",
        "\n",
        "truncation=True: 너무 길면 512까지만 자름\n",
        "\n",
        "결과는 다음과 같은 dict 형태\n",
        "\n",
        "e.g.,\n",
        "```python\n",
        "{\n",
        "  'input_ids': [...숫자들...],\n",
        "  'attention_mask': [...1, 1, 1, 0, 0, 0...]  # pad된 부분은 0\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aNSjE332-NM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 모델 학습을 위한 collator(데이터 배치 준비 도우미) 준비"
      ],
      "metadata": {
        "id": "bIHgogDz_bwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "# min=False -> GPT,LLAMA 같은 casual language model 학습에 적합"
      ],
      "metadata": {
        "id": "FsdKqfro_isP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Trainer 설정"
      ],
      "metadata": {
        "id": "1zRpLEQqAIua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TrainingAruguments**\n",
        "\n",
        ": 모델 학습과 관련된 다양한 하이퍼파라미터와 설정들을 모아놓은 객체. Trainer에게 어떻게 학습할지 알려주는 설정 모음집이라 할 수 있다."
      ],
      "metadata": {
        "id": "8E7NX1AqBE_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Trainer 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/gdrive/MyDrive/models/checkpoints\", # 학습 체크포인트(모델 가중치 등)를 저장할 폴더 경로\n",
        "    per_device_train_batch_size=2, # 학습 시 GPU 1대당 사용할 배치 사이즈\n",
        "    per_device_eval_batch_size=4, # 평가 시 GPU 1대당 사용할 배치 사이즈\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=3, # 총 학습 Epoch 수(3번 데이터셋을 반복 학습)\n",
        "    logging_dir=\"./logs\", # 로그(학습 진행상황 등)를 저장할 디렉토리\n",
        "    learning_rate=5e-5,  # 학습률 설정\n",
        "    weight_decay=0.01,  # L2 정규화\n",
        "    lr_scheduler_type=\"linear\",  # 학습률 스케줄러\n",
        "    logging_steps=10, # 몇 스텝마다 로그를 기록할지(여기선 10스텝마다)\n",
        "    save_strategy=\"steps\", # 체크포인트 저장 주기(여기선 step마다 저장)\n",
        "    #evaluation_strategy=\"epoch\", # 평가 주기(여기선 epoch마다 평가)\n",
        "    fp16=True, # 16-bit half precision 사용 여부(속도 향상 및 메모리 절약)\n",
        "    save_steps=50,\n",
        "    save_total_limit=2, # 저장할 체크포인트 최대 개수(초과 시 오래된 것부터 삭제)\n",
        "    report_to=\"wandb\"  # WandB에 결과 보고\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, # 학습할 PyTorch 모델\n",
        "    args=training_args, # TrainingArguments 객체(학습 설정)\n",
        "    train_dataset=tokenized_train, # 학습용 데이터셋\n",
        "    eval_dataset=tokenized_val, # 평가용 데이터셋\n",
        "    tokenizer=tokenizer, # 토크나이저(토큰 ID 변환 및 디코딩 등에 사용)\n",
        "    data_collator=data_collator, # 배치 데이터를 만들 떄 자동으로 padding 등을 처리하는 함수\n",
        ")"
      ],
      "metadata": {
        "id": "4bqmMIPXpF6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# 학습 시작\n",
        "# trainer.train(resume_from_checkpoint=True)\n",
        "# wanb.init\n",
        "trainer.train()\n",
        "\n",
        "# 모델 저장\n",
        "model.save_pretrained(\"/content/gdrive/finetuned-kollama-dialect\")\n",
        "tokenizer.save_pretrained(\"/content/gdrive/finetuned-kollama-dialect\")"
      ],
      "metadata": {
        "id": "r09eHRnAAYgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_path = \"/content/gdrive/MyDrive/finetuned_kollama_dialect\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "model.eval()  # 평가 모드로 전환 (추론할 때)\n"
      ],
      "metadata": {
        "id": "y2DYGW1jE6wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"User: 덥긴한데 벌써부터 에어컨 틀면 전기세가 느무 많이 나오겠제?\\nBot:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
      ],
      "metadata": {
        "id": "RYVL2CoUrVzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=60,         # 너무 길게 생성 못하도록 제한\n",
        "        min_length=10,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.8,\n",
        "        eos_token_id=tokenizer.eos_token_id,  # EOS에서 멈춤\n",
        "    )\n",
        "\n",
        "full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# 프롬프트 이후의 응답만 추출\n",
        "response = full_text[len(prompt):].strip().split(\"\\nUser\")[0].strip()\n",
        "sentences = re.findall(r'[^.!?]*[.!?]', response, re.UNICODE)\n",
        "\n",
        "# 양쪽 공백 제거\n",
        "clean_sentences = [s.strip() for s in sentences]\n",
        "\n",
        "# 완성된 문장들만 출력\n",
        "for sentence in clean_sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "0t2zw5K8rYVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS"
      ],
      "metadata": {
        "id": "FJCYs8eqZwIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "\n",
        "if clean_sentences:\n",
        "    tts = gTTS(text=response, lang='ko')\n",
        "    tts.save(\"output.mp3\")\n",
        "    display(Audio(\"output.mp3\", autoplay=True))\n",
        "else:\n",
        "    print(\"생성된 문장이 없습니다.\")"
      ],
      "metadata": {
        "id": "6O9F3Oz8ZVBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7r1dmjQjIanf"
      }
    }
  ]
}