# -*- coding: utf-8 -*-
"""HeardU

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tTyLRj7pAKgt4uC3w7Jairti0FXKZkF1
"""

!pip install numpy==1.24.4


from google.colab import drive
drive.mount('/content/drive')


!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118
!pip install git+https://github.com/openai/whisper.git
!pip install datasets librosa accelerate transformers scikit-learn


import torch
import whisper
import torchaudio
import json
from torch.utils.data import Dataset, DataLoader
from transformers import get_scheduler
from tqdm import tqdm
import os
from sklearn.model_selection import train_test_split


class ContextDataset(Dataset):
    def __init__(self, file_list):
        self.data = file_list

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        audio_path, label, context = self.data[idx]
        audio, sr = torchaudio.load(audio_path)
        if sr != 16000:
            resampler = torchaudio.transforms.Resample(sr, 16000)
            audio = resampler(audio)
        audio = audio.squeeze()

        full_label = context + " " + label

        return audio, full_label


device = "cuda" if torch.cuda.is_available() else "cpu"
model = whisper.load_model("tiny").to(device)


optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
loss_fn = torch.nn.CrossEntropyLoss()


'''
#파일 개수 190개
# wav 파일 폴더 경로
wav_folder = "/content/drive/MyDrive/인지응팀플/content/wav_files/노인남여_노인대화07_F_1522434093_60_경상_실내"
# json 파일 폴더 경로
json_folder = "/content/drive/MyDrive/인지응팀플/content/json_files/노인남여_노인대화07_F_1522434093_60_경상_실내"
'''
# 파일개수 480개
# wav 파일 폴더 경로
wav_folder = "/content/drive/MyDrive/인지응팀플/content/wav_files/충청"
# json 파일 폴더 경로
json_folder = "/content/drive/MyDrive/인지응팀플/content/json_files/충청"


file_list = []
i = 0
for filename in os.listdir(wav_folder):
    i += 1
    if filename.endswith(".wav"):
        base_name = filename[:-4]
        audio_path = os.path.join(wav_folder, filename)
        json_path = os.path.join(json_folder, base_name + ".json")

         # json 파일 존재
        if not os.path.exists(json_path):
            print("매치되는 json파일X")
            continue

        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            label = data["발화정보"]["stt"]
            city = data["대화정보"].get("cityCode", "")
            env = data["대화정보"].get("recrdEnvrn", "")
            theme = data["대화정보"].get("convrsThema", "")
            context = f"{theme.strip()}, {city.strip()}, {env.strip()} 대화입니다."

        file_list.append((audio_path, label, context))
        if i%10 == 0:
            print(f"{i} : {filename} 추가 완료")

if len(file_list) < 3:
    raise ValueError("데이터셋 없슈슈")


train_data, temp_data = train_test_split(file_list, test_size=0.3, random_state=42)
valid_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)

train_loader = DataLoader(ContextDataset(train_data), batch_size=1, shuffle=True)
valid_loader = DataLoader(ContextDataset(valid_data), batch_size=1, shuffle=False)

####학습
num_epochs = 1
model.train()
tokenizer = whisper.tokenizer.get_tokenizer(multilingual=model.is_multilingual)
losslst = []

for epoch in range(num_epochs):
    train_loss_epoch = 0
    loop = tqdm(train_loader, leave=True)
    for audio, label in loop:
        audio = audio.to(device)
        audio = whisper.pad_or_trim(audio)
        mel = whisper.log_mel_spectrogram(audio).to(device)

        tokens = tokenizer.encode(label[0])
        tokens = torch.tensor(tokens, device=device).unsqueeze(0)

        out = model(mel, tokens[:, :-1])

        loss = loss_fn(
            out.view(-1, out.size(-1)),
            tokens[:, 1:].reshape(-1)
        )

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss_epoch += loss.item()
        loop.set_description(f"Epoch : {epoch+1}")
        loop.set_postfix(loss=loss.item())
        losslst.append(loss.item())
        print(loss.item(),"\n")

    # 검증 
    model.eval()
    val_losses = []
    with torch.no_grad():
        for audio, label in valid_loader:
            audio = audio.to(device)
            audio = whisper.pad_or_trim(audio)
            mel = whisper.log_mel_spectrogram(audio).to(device)
            tokens = tokenizer.encode(label[0])
            tokens = torch.tensor(tokens, device=device).unsqueeze(0)
            out = model(mel, tokens[:, :-1])
            val_loss = loss_fn(out.view(-1, out.size(-1)), tokens[:, 1:].reshape(-1))
            val_losses.append(val_loss.item())

    val_loss_mean = sum(val_losses)/len(val_losses)
    print(f"\n Epoch : {epoch+1} | Validation Loss: {val_loss_mean:.4f}\n")
    model.train()

##저장장
save_path = "/content/drive/MyDrive/인지응팀플/0522_validation.pt"
os.makedirs(os.path.dirname(save_path), exist_ok=True)
torch.save(model.state_dict(), save_path)
print(f"\n모델 저장 {save_path}")
